# ğŸ§  GPT2 - GQA: GeraÃ§Ã£o de Texto com Arquitetura Customizada
Este projeto tem como objetivo treinar um modelo de linguagem baseado no **GPT-2** para geraÃ§Ã£o de texto em portuguÃªs, utilizando obras de **Machado de Assis** como corpus principal. A arquitetura foi adaptada para tarefas de geraÃ§Ã£o com avaliaÃ§Ã£o qualitativa e checkpoints salvos ao longo do treinamento.

## ğŸ“ Estrutura do projeto

```
deeplearning-final/
â”œâ”€â”€ notebooks/              # Notebooks de experimentaÃ§Ã£o e visualizaÃ§Ã£o
â”œâ”€â”€ scripts/                # Scripts de ingestÃ£o e prÃ©-processamento
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gpt_2/              # Arquitetura customizada do modelo
â”‚   â””â”€â”€ utils/              # FunÃ§Ãµes auxiliares
â”œâ”€â”€ train/                  # Loop de treinamento e avaliaÃ§Ã£o
â”œâ”€â”€ data/                   # Corpus limpo e dividido
â”œâ”€â”€ checkpoints/            # Modelos salvos por Ã©poca
â”œâ”€â”€ logs/                   # Logs de treinamento
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ README.md               # Este arquivo
```

## ğŸš€ ExecuÃ§Ã£o

O treinamento estÃ¡ sendo realizado em um notebook do Kaggle, aproveitando os recursos gratuitos de GPU. Para reproduzir:

1. Acesse o notebook no Kaggle
   - [GPT2 - MHA (Baseline)](https://www.kaggle.com/code/losout0/gpt-multihead)
   - [GPT2 - GQA](https://www.kaggle.com/code/losout0/gpt-gqa)
   - [ComparaÃ§Ã£o GQA e MHA](https://www.kaggle.com/code/andrefelipealmeida/fork-of-compara-o-gpt-gqa-e-multihead)
2. Execute as cÃ©lulas na ordem para:
   - Baixar e limpar os dados
   - Inicializar o modelo
   - Treinar e salvar checkpoints

## ğŸ“¦ DependÃªncias
Instale os pacotes necessÃ¡rios com:

```bash
pip install -r requirements.txt
```
**Principais bibliotecas:**
- **torch** â€“ Treinamento do modelo
- **tiktoken** â€“ TokenizaÃ§Ã£o eficiente
- **requests, tqdm** â€“ Download e progresso
- **transformers** â€“ Base para o GPT-2

## ğŸ“š Dados
Os textos foram extraÃ­dos do [Projeto Gutenberg](https://www.gutenberg.org/) e processados para remover metadados, normalizar pontuaÃ§Ã£o e dividir em parÃ¡grafos com tamanho mÃ­nimo.

## ReferÃªncias
1. [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)
2. [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

3. [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch)
